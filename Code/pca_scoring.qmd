---
title: "Review Data Cleaning and PCA Modeling"
format: docx
editor: visual
---

## Preliminaries

Import libraries.

```{r, message=FALSE, warning=FALSE}
library(tidyverse)
library(factoextra)
```

Import reviewer-review level data.

```{r}
# this is a big data set! This takes a minute or two
# we are subsetting this to speed up initial computations
review <- read_csv("../Data/final_modeling_df.csv") # |> slice(1:1000000)
```

## Data Cleaning + Visualization

Check the structure of the data. Check for duplicate user IDs based on `review_count`.
```{r}
# Check data structure and identify reviewer vs review level variables
cat("Dataset dimensions:", dim(review), "\n")
cat("Unique users:", length(unique(review$user_id)), "\n")
cat("Unique businesses:", length(unique(review$business_id)), "\n")

# Check for duplicate user-level information based on review_count
review |>
  group_by(user_id) |>
  summarise(
    unique_review_counts = n_distinct(review_count),
    .groups = 'drop'
  ) |>
  summarise(
    users_with_varying_review_count = sum(unique_review_counts > 1),
  )
```

Check summary statistics.

```{r}
summary(review)
```

Compute the sum of `NA` values in each column. Most of the NA values are coming from product attributes.

```{r}
apply(review, 2, function(x) sum(is.na(x)))
```

View some rows with missing values of product_attributes.

```{r} 
review |>
  filter(is.na(product_attributes)) |>
  head(10)
```

View rows with missing values of review_count.

```{r}
review |>
  filter(is.na(review_count)) |>
  head(10)
```

Deal with missing values. We impute empty strings for product attributes and drop rows with missing values in other variables. (Will revisit data cleaning to figure out why these missing values exist).

```{r}
review <- review |> 
  mutate(
    # Impute missing product attributes with empty strings
    product_attributes = ifelse(is.na(product_attributes), "", product_attributes),
  )

review <- review |>
  filter(if_all(everything(), ~!is.na(.)))
```

View the data. Some variables are reviewer-level (they have the same value across all reviewer-specific observations) and some are review-level.

```{r}
review |>
  arrange(user_id)
```

Compute summaries of review-level variables for each reviewer and additional features of interest.

```{r}
review <- review |>
  mutate(date = as.Date(date)) |>
  arrange(user_id, date) |>
  group_by(user_id) |>
  mutate(
         # compute the average review length for each reviewer 
         avg_review_length = mean(review_length),
         # compute the proportion of positive reviews written by a given reviewer
         avg_sentiment = mean(sentiment == "POSITIVE"),
         # Consistency metrics
         review_length_variance = var(review_length),
         time_between_reviews_variance = var(diff(date)),
         # Suspicious patterns
         review_burst_count = sum(diff(date) < 1),
         # proportion of reviews with 1 or 5 star ratings
         extreme_rating_proportion = mean(stars %in% c(1, 5))) |>
  ungroup() 
```

Check for missing values in the newly created variables.
```{r}
apply(review, 2, function(x) sum(is.na(x)))
```

Replace missing values for `review_length_variance` and `time_between_reviews_variance` with 0, as these metrics are not applicable for users with only one review.

```{r}
review <- review |>
  mutate(
    review_length_variance = ifelse(is.na(review_length_variance), 0, review_length_variance),
    time_between_reviews_variance = ifelse(is.na(time_between_reviews_variance), 0, time_between_reviews_variance)
  )
```

Select reviewer-level variables and pivot to long format.

```{r}
# create data frame with user-level variables and ID
review_user <- review |>
  select(user_id, 
         active_days, 
         review_count,
         review_frequency,
         time_between_reviews_variance,
         review_burst_count,
         useful_user,
         funny_user,
         cool_user,
         star_rating_variance,
         extreme_rating_proportion, 
         avg_review_length,
         review_length_variance,
         avg_sentiment,
         account_age_days,
         average_stars)

long_user <- review_user |>
  pivot_longer(names_to="Feature", values_to="Value", -user_id)
```

Visualize feature distributions for reviewer-level characteristics. Some users have really extreme values for the votes like 'cool', 'funny', and 'useful'. Review count and frequency are also strongly right-skewed.

```{r fig.width=7, fig.height=5}
long_user |>
  ggplot(aes(x=Value)) +
  geom_density() +
  facet_wrap(~Feature, scales='free')
```

Use a log transformation to address the skewness. Use an offset of one for all transformed variables.

```{r}
# Consider Box-Cox transformation instead of log for heavily skewed variables
review_user_transformed <- review_user %>%
  mutate(across(
    .cols = c("active_days",
              "avg_review_length",
              "cool_user",
              "funny_user",
              "review_burst_count",
              "review_count",
              "review_frequency",
              "review_length_variance",
              "star_rating_variance",
              "time_between_reviews_variance",
              "useful_user"),
    .fns = log1p
  ))
```

Redefine `long_user` using the log-transformed data.

```{r}
long_user <- review_user_transformed |>
  pivot_longer(names_to="Feature", values_to="Value", -user_id)
```

Plot feature distributions again. They are slightly improved.

```{r fig.width=7, fig.height=5}
long_user |>
  ggplot(aes(x=Value)) +
  geom_density() +
  facet_wrap(~Feature, scales='free')
```

## PCA

Compute principal components. We center and scale all features to weight them equally.

```{r}
# the -1 column subset excludes the user ID
pca <- prcomp(review_user_transformed[, -1], center=TRUE, scale=TRUE)
```

View PCA results. Three eigenvectors have eigenvalues greater than 1 (rule of thumb) and three components are able to explain 72% of the variance of the data, which is pretty high for only three components.

```{r}
summary(pca)
```

Eigenvalue scree plot. The red line indicates the eigenvalue = 1 threshold, which is a common rule of thumb for determining the number of components to retain. Components with eigenvalues greater than 1 are generally considered significant.

```{r}
# Calculate eigenvalues from PCA results
eigenvalues <- pca$sdev^2

# Create a scree plot
tibble(
  PC = 1:length(eigenvalues),
  Eigenvalue = eigenvalues,
  Proportion = eigenvalues / sum(eigenvalues),
  Cumulative = cumsum(Proportion)
) |>
  ggplot(aes(x = PC, y = Eigenvalue)) +
  geom_point() +
  geom_line() +
  geom_hline(yintercept = 1, linetype = "dashed", color = "red") +
  labs(title = "Scree Plot", subtitle = "Red line shows eigenvalue = 1 threshold")
```

Plot the distribution of PC1.

```{r}
as_tibble(pca$x[, 1]) |>
  ggplot(aes(x = value)) +
  geom_density()
```

Plot directional loadings for PC1 and PC2. 
```{r}
# To visualize component loadings more clearly
fviz_pca_var(pca, col.var = "contrib", 
             gradient.cols = c("#00AFBB", "#E7B800", "#FC4E07"),
             repel = TRUE)
```

Visualize loadings for the first three components. The loadings indicate how much each feature contributes to each principal component. Positive loadings indicate a positive relationship with the component, while negative loadings indicate a negative relationship.

```{r}
# Examine loadings for first 3 components
loadings_df <- as_tibble(pca$rotation[, 1:3], rownames = "Feature")

# Visualize loadings
loadings_df |>
  pivot_longer(cols = -Feature, names_to = "PC", values_to = "Loading") |>
  ggplot(aes(x = reorder(Feature, Loading), y = Loading, fill = Loading > 0)) +
  geom_col() +
  facet_wrap(~PC) +
  coord_flip() +
  labs(title = "PCA Loadings for First 3 Components")
```

View the first three eigenvectors.

-   E1: The only variable with a positive loading is `review_frequency`. Someone who leaves very frequent reviews will have a more positive value for PC1. However, the `review_count` and community indications of review quality, i.e., `useful_user`, `funny_user`, and `cool_user` all have large negative loadings. So, someone with a large number of reviews that have received favorable feedback from the community will have a large negative value for PC1, especially if their review frequency is low. This seems pretty interesting because PC1 on its own may be a decent differentiator of reviewer quality/trust - a bot that leaves a large number of reviews but does so with high frequency will not have a very negative value for PC1 especially if those bot reviews don't receive positive community feedback. Keep in mind that since we scaled the data, the values of the variables are standard deviations, which incorporate information for each user relative to other users, i.e., it's not just about having a high amount of positive community feedback or a high review count, it's about having high amounts relative to the rest of the user population.

Use min-max scaling to transform PC1 values to 0 to 1 scale. 

```{r}
# Min-max scaling for PC1
trust_scores <- (pca$x[, 1] - min(pca$x[, 1])) / (max(pca$x[, 1]) - min(pca$x[, 1]))
```

Plot trust scores.

```{r}
tibble(score = trust_scores) |>
  ggplot(aes(x = score)) +
  geom_density()
```

What do some of the people look like at either end of this distribution?

```{r}
# add trust score variable to user level data
review_user <- review_user |>
  mutate(trust_scores = trust_scores)
```

Top 5 trustworthy users.

```{r}
review_user |>
  slice_max(trust_scores, n=5)
```

Top 5 untrustworthy users.

```{r}
review_user |>
  slice_min(trust_scores, n=5)
```